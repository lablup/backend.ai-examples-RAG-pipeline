export HF_MODEL_REPO="RichardErkhov/MLP-KTLim_-_llama-3-Korean-Bllossom-8B-awq" # Hugging Face model repository URL
export MODEL_NAME="llama-3-Korean-Bllossom-8B-awq" # Your LLM model name (e.g. llama-3-Korean-Bllossom-8B
export MODEL_PATH="/models" # Path to the model directory (e.g. /models
export TENSOR_PARALLEL_SIZE=1 # Tensor parallel size (e.g. 1)
export MODEL_PORT=8000 # Port for the model service (e.g. 8000)
export MAX_MODEL_LEN=4096 # Maximum model context length (e.g. 4096)
export MAX_NUM_BATCHED_TOKENS=8000 # Maximum number of batched tokens for inference (e.g. 2048, 1024, ...)
export D_TYPE="float16" # Data type for model inference. (e.g. float16, bfloat16, int8, ...)
export MAX_NUM_SEQ=2 # Maximum number of sequences for batch inference (e.g. 16, 8, ...)
export GPU_MEMORY_UTILIZATION=0.8 # GPU memory utilization limit (e.g. 1, 0.85, ...)
