# Environment Configuration Template for Korean RAG Sequential Tasks
# Copy this file to .env and modify the values as needed

# =============================================================================
# DIRECTORY PATHS
# =============================================================================
DATA_DIR=/home/work/RAG-example/data/sample # Directory containing PDF files
CACHE_DIR=/home/work/RAG-example/cleaned # Directory for cleaned text files
PROCESSED_DIR=/home/work/RAG-example/processed # Directory for processed documents
INDEX_DIR=/home/work/RAG-example/indexes # Directory for FAISS and BM25 indexes
QUERY_DIR=/home/work/RAG-example/query_results # Directory for query processing results
RESPONSE_DIR=/home/work/RAG-example/responses # Directory for final responses
EVAL_DATASET_PATH=/home/work/RAG-example/tasks/task7_evaluation/sample/evaluation_dataset.json # Path to evaluation dataset JSON file (Task 7)
RESULTS_DIR=/home/work/RAG-example/tasks/task7_evaluation/eval_results # Directory for evaluation results (Task 7)

# =============================================================================
# MODEL ENDPOINTS AND AUTHENTICATION
# =============================================================================
# LLM Model Endpoint (Task 5)
MODEL_ENDPOINT= # Your LLM model endpoint URL
MODEL_NAME= # Your LLM model name
API_KEY= # (Optional) Your API key for the LLM service

# HuggingFace Token (for accessing private models or higher rate limits)
HUGGINGFACE_TOKEN= # your-hugging-face-token-here

# =============================================================================
# EMBEDDING CONFIGURATION (Task 2)
# =============================================================================
# Embedding Model Selection
EMBED_MODEL= # embedding model name from hugging face (e.g. nlpai-lab/KURE-v1)

# =============================================================================
# DOCUMENT PROCESSING (Task 2)
# =============================================================================
CHUNK_SIZE=300                     # Text chunk size for splitting
CHUNK_OVERLAP=60                   # Overlap between consecutive chunks

# =============================================================================
# RETRIEVAL CONFIGURATION (Task 4)
# =============================================================================
TOP_K=32                          # Number of documents to retrieve initially
FINAL_K=6                         # Number of final documents after reranking
RERANK_POOL=40                    # Maximum documents to consider for reranking

# Reranking Configuration
USE_RERANK=true                   # Enable/disable reranking
RERANK_MODEL=  # Cross-encoder model for reranking (e.g. BAAI/bge-reranker-v2-m3)

# =============================================================================
# QUERY CONFIGURATION (Task 4 & 5)
# =============================================================================
QUERY="Backend.AI 에서 제한된 사람, 서비스만 사용자가 모델 서비스에 접근할 수 있게 하려면 어떻게 하는지 모델 서비스 생성과 함께 자세히 알려줘."

# =============================================================================
# RESPONSE GENERATION (Task 5)
# =============================================================================
TEMPERATURE=0.2                   # LLM temperature for response generation
MAX_TOKENS=3000                   # Maximum tokens for response
MODEL_CTX_LIMIT=4096             # Model context window limit
TOKENIZER_MODEL= # Tokenizer for token counting (e.g. MLP-KTLim/llama-3-Korean-Bllossom-8B)
PER_DOC_CAP=320                  # Maximum tokens per document in context
SAFETY_MARGIN=256                # Safety margin for token counting

# System Prompts (customize as needed)
RAG_SYSTEM_PROMPT="당신은 Backend.AI 전문가이며, 근거 중심 어시스턴트입니다. 아래 컨텍스트의 근거에 한해서만 답변하세요. 근거가 부족하면 '근거 부족'이라고 말하세요. 마지막에 반드시 출처 파일명, 소제목을 함께 제공하세요. 순서가 필요하면 순서를 정확하게 언급하세요."
NON_RAG_SYSTEM_PROMPT="당신은 Backend.AI 전문 어시스턴트입니다. 간결하고 사실 기반으로 답변하세요."

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================
# Logging Level
LOG_LEVEL=INFO                    # DEBUG, INFO, WARNING, ERROR

# Performance Settings
RERANK_DEVICE=cpu               # Device for reranking: cuda, cpu
RERANK_BATCH=32                  # Batch size for reranking

# Index Saving (Task 3)
SAVE_INDEX=0                     # 1 to save indexes, 0 to skip (recommended: 0)